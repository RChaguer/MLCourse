{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IF240 - Apprentissage et deep learning\n",
    "\n",
    "## Practice 2: Naive Bayes, Evaluation Metrics, SVM \n",
    "\n",
    "By Aur√©lie Bugeau\n",
    "Credits:  Vincent Lepetit, Varun Kumar, Mohit Deshpande"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives \n",
    "The objective of the practice is to clssifiy emails from a dataset as spam or non-spam. \n",
    "You will compare diffrent classification methods: Naive Bayes and SVM, ansd implement several validation metrics\n",
    "\n",
    "### Libraries\n",
    "The code needs to import the following libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Dataset\n",
    "\n",
    "#### Presentation and Loading\n",
    "The dataset used here contained 747 spam and 4825 non-spam (i.e. ham) mails. \n",
    "Emails in the corpus have been already pre-processed in the following ways:\n",
    "Removal of stop words (and, the, of, etc) and lemmatization (inludes, included, include are now all considered as include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     4825\n",
      "spam     747\n",
      "Name: Category, dtype: int64\n",
      "  Category                                            Message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Open the dataset\n",
    "mails = pd.read_csv(\"spamham.csv\")\n",
    "#count the number of spam/ham mails\n",
    "count = mails['Category'].value_counts()\n",
    "print(count)\n",
    "print(mails.head(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation sets\n",
    "Split the dataset into training and evaluation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split data as train and evaluation sets. \n",
    "msk = np.random.rand(len(mails)) < 0.8\n",
    "training_set = mails[msk]\n",
    "testing_set = mails[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Naive Bayes Classification\n",
    "The classifier must be able to predict the label based on the text by implementing the following pseudo code:\n",
    "\n",
    "`\n",
    "if (P('ham' | message ) > P( 'spam' | message )) {\n",
    "  return ‚Äòham‚Äô;\n",
    "} \n",
    "else {\n",
    "  return ‚Äòspam‚Äô;\n",
    "}\n",
    "`\n",
    "\n",
    "where\n",
    "$$ P(ham | message)~=~ {\\rm Probability ~that~ email~ is ~ham~ given~ that~ it~ has~ certain~ features~} $$\n",
    "The features can for instance be a set of given words.\n",
    "$$ P(spam | message)~=~ {\\rm Probability ~that~ email~ is ~spam~ given~ that~ it~ has~ certain~ features~} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "Apply the Naive Bayes formula in the following code to implement a classifier. You will consider that:\n",
    "$$P(message | spam) = P(word1 | spam) * P(word2 | spam) *...$$\n",
    "\n",
    " \n",
    "_Note:_ if a word in the testing dataset is not present in the training dataset, you may encounter problems as $P(new | ham)$ or $P(new | spam)$ will be 0 making all product equal to 0.\n",
    "To solve this problem, we should take log on both sides. New pseudo code will be\n",
    "\n",
    "`\n",
    "if (log(P('ham' | message )) > log(P('spam' | message))) {\n",
    "  return ‚Äòham‚Äô;\n",
    "} else {\n",
    "  return ‚Äòspam‚Äô;\n",
    "}\n",
    "`\n",
    "\n",
    "Then \n",
    "$$ log(P(message| spam)) =  log(P(word1 | spam)) + log(P(word2 | spam)) ‚Ä¶$$\n",
    "\n",
    "But the problem is still not solved. If the classifier encounters a new word that is not present in our training data sets then P(new-word | category) will be 0 and log(0) is undefined. To solve this problem, you must use Laplace smoothing:\n",
    "\n",
    "$$P(word1 | spam) = \\frac{{\\rm number~ of ~}word1 {\\rm~belonging~ to ~category~ spam + 1}}{{\\rm  number ~ of ~words~ belonging~ to ~spam ~}+{ \\rm ~number ~of~ distinct ~words~ in ~training ~datasets~}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tokenize a string into words    \n",
    "def tokenize(text):\n",
    "    return re.split(\"\\W+\", str(text))\n",
    "    \n",
    "class SpamDetectorNB(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.numberOfmessages = {} \n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.word_index = {}\n",
    "        self.vocab = set()\n",
    "    \n",
    "\n",
    "    # compute log class priors log(ùëÉ(‚Ñéùëéùëö)) and log(ùëÉ(spùëéùëö))  \n",
    "    #by counting up how many spam/ham messages are in our dataset and dividing by the total number\n",
    "    def log_priors(self, training_set):\n",
    "        counter = Counter(training_set['Category'])\n",
    "        total = counter['spam'] + counter['ham']\n",
    "        self.log_class_priors['spam'] = math.log(counter['spam']/total)\n",
    "        self.log_class_priors['ham'] = math.log(counter['ham']/total)\n",
    "        \n",
    "     \n",
    "    #Count how many times each word appears in a text. \n",
    "    #Returns a dictionary that contain for each word indicates the number of times it appears in text. \n",
    "    def get_word_counts(self, text):\n",
    "        word_set = tokenize(text)\n",
    "        word_counts = Counter(word_set)\n",
    "        return word_counts\n",
    "    \n",
    "    #Create a dictionary (a vocabulary of words)\n",
    "    #and count words frequency for spam and ham separately\n",
    "    def get_word_frequency(self, training_set):\n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    "        for _, mail in training_set.iterrows():\n",
    "            label = mail['Category']\n",
    "            text = mail['Message']\n",
    "            #Tokenize each message into words.\n",
    "            counts = self.get_word_counts(tokenize(text))\n",
    "            for word, count in counts.items():\n",
    "                if word not in self.vocab:\n",
    "                    self.word_index[word] = len(self.vocab)\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[label]:\n",
    "                    self.word_counts[label][word] = 0.0\n",
    "                self.word_counts[label][word] += count\n",
    "                \n",
    "                \n",
    "    #compute all necessary features\n",
    "    def train(self, training_set):\n",
    "        self.log_priors(training_set)\n",
    "        self.get_word_frequency(training_set)\n",
    "        \n",
    "        \n",
    "    def predict(self, testing_set):\n",
    "        result = []\n",
    "        for _, mail in testing_set.iterrows():\n",
    "            label = mail['Category']\n",
    "            text = mail['Message']\n",
    "            \n",
    "            #Tokenize each message into words.\n",
    "            counts = self.get_word_counts(tokenize(text))\n",
    "            \n",
    "            #Initialize ùëôùëúùëî(ùëÉ(spam|message)) and ùëôùëúùëî(ùëÉ(ham|message))  according to log priors\n",
    "            log_pm_spam = 0\n",
    "            log_pm_ham = 0\n",
    "\n",
    "            #For each message, compute ùëôùëúùëî(ùëÉ(ùëöùëíùë†ùë†ùëéùëîùëí|ùë†ùëùùëéùëö)) and ùëôùëúùëî(ùëÉ(ùëöùëíùë†ùë†ùëéùëîùëí|ùë†ùëùùëéùëö)) \n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue         \n",
    "                #For each word compute log(P(w/spam)) and log(P(w/ham)) \n",
    "                if(word not in self.word_counts['spam']):\n",
    "                    log_pw_spam = np.log(1 / (len(self.word_counts['spam']) + len(self.vocab)))\n",
    "                else:\n",
    "                    log_pw_spam = np.log((self.word_counts['spam'][word] +  1) / (len(self.word_counts['spam']) + len(self.vocab)))\n",
    "\n",
    "                if(word not in self.word_counts['ham']):\n",
    "                    log_pw_ham = np.log(1 / (len(self.word_counts['ham'])+len(self.vocab)))\n",
    "                else:\n",
    "                    log_pw_ham = np.log((self.word_counts['ham'][word]+1)/(len(self.word_counts['ham']) + len(self.vocab)))\n",
    "    \n",
    "                #Update ùëôùëúùëî(ùëÉ(ùëöùëíùë†ùë†ùëéùëîùëí|ùë†ùëùùëéùëö)) and ùëôùëúùëî(ùëÉ(ùëöùëíùë†ùë†ùëéùëîùëí|ùë†ùëùùëéùëö)) \n",
    "                log_pm_spam += log_pw_spam\n",
    "                log_pm_ham += log_pw_ham\n",
    "                \n",
    "            #decide spam or ham\n",
    "            log_p_spam_m = log_pm_spam + self.log_class_priors['spam']\n",
    "            log_p_ham_m = log_pm_ham + self.log_class_priors['ham']\n",
    "\n",
    "            if(log_p_spam_m >= log_p_ham_m):\n",
    "                result.append([label,'spam'])\n",
    "            else:\n",
    "                result.append([label,'ham'])\n",
    "\n",
    "        return result             \n",
    "    \n",
    "sd = SpamDetectorNB()\n",
    "sd.train(training_set)\n",
    "result = sd.predict(testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - Accuracy and confution matrix\n",
    "Compute the precision, recall, accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'tn': 900, 'tp': 115, 'fn': 42})\n",
      "Precision: 1.0000\n",
      "Recall: 0.7325\n",
      "Accuracy : 0.9603\n",
      "Confusion matrix using SpamDetectorNB:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def status_detect(st):\n",
    "    if (len(st) != 2):\n",
    "        return 'error'\n",
    "    if (st[0] == st[1]):\n",
    "        if (st[0] == 'spam'):\n",
    "            return 'tp'\n",
    "        return 'tn'\n",
    "    else:\n",
    "        if (st[0] == 'spam'):\n",
    "            return 'fn'\n",
    "        return 'fp'\n",
    "\n",
    "stats = [status_detect(m) for m in result]\n",
    "g_counter = Counter(stats)\n",
    "\n",
    "total = len(result)\n",
    "tp = g_counter['tp']\n",
    "fp = g_counter['fp']\n",
    "fn = g_counter['fn']\n",
    "tn = g_counter['tn']\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "accuracy = (tp + tn) / total  \n",
    "print(g_counter)\n",
    "print(\"Precision: {0:.4f}\".format(precision))\n",
    "print(\"Recall: {0:.4f}\".format(recall))\n",
    "print(\"Accuracy : {0:.4f}\".format(accuracy))\n",
    "print(\"Confusion matrix using SpamDetectorNB:\")\n",
    "cf_matrix = [[tp/total, fp/total], [fn/total, tn/total]]\n",
    "\n",
    "cf_cm = pd.DataFrame(cf_matrix, index = ['spam', 'ham'],\n",
    "                  columns = ['spam', 'ham'])\n",
    "plt.figure()\n",
    "sn.heatmap(cf_cm, annot=True, cbar=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 - Naive Bayes with Scikit-learn library\n",
    "The Scikit-learn library proposes many functions for machine learning.  Study the documentation of the  MultinomialNB function and apply it for spam detection.\n",
    "\n",
    "You will need to convert the dataset into arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def data_to_vector(darray):\n",
    "    X = training_set.values[:, 1]\n",
    "    Y = training_set.values[:, 0]\n",
    "    \n",
    "    cv = CountVectorizer()\n",
    "    word_count_vector = cv.fit_transform(X)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector) \n",
    "    tfidf_count = tfidf_transformer.transform(word_count_vector)\n",
    "    \n",
    "    return tfidf_count\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 - Evaluation with Scikit-learn library\n",
    "The Scikit-learn library also proposes  functions to evaluate machine learning methods. Apply them to the spam detection problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cmMNb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-dd24aeda91c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#for visualisation of the confusion matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# for plot styling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmMNb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# xticklabels=,yticklabels=\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cmMNb' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "#COMPLETE\n",
    "\n",
    "#for visualisation of the confusion matrix\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "sns.heatmap(cmMNb.T, square=True, annot=True, fmt='d', cbar=False) # xticklabels=,yticklabels=\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 - SVM Scikit-learn library\n",
    "Apply SVM with a linear kernel to the spam dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "#COMPLETE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
