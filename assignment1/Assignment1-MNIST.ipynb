{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"colab":{"name":"Assignment1-MNIST.ipynb","provenance":[],"collapsed_sections":["35VZ2RmIgKQt","LgDZhqvxgKQu","1JSxEiE0gKQ4","AXrTKTwngKQ5","sRzgtFdfgKQ6","o5tIogXPgKQ7"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"pNnJICs0gKQW"},"source":["# IF240 - Apprentissage et deep learning\n","\n","## Assignment 1: K-Means, Naive Bayes, SVMs and CNNs"]},{"cell_type":"markdown","metadata":{"id":"VywMttxcgKQe"},"source":["### Objectives \n","\n","The objective of this assignment is to apply different classification algorithms for the application of handwritten digits recognition."]},{"cell_type":"markdown","metadata":{"id":"L0h5f98RgKQf"},"source":["# PART 0 - Preparing the dataset"]},{"cell_type":"markdown","metadata":{"id":"TTizhf3_gKQh"},"source":["In this practice, you will experiment with the well-known MNIST dataset"]},{"cell_type":"code","metadata":{"id":"2EQ1DB_KgKQi","executionInfo":{"status":"ok","timestamp":1619273749874,"user_tz":-120,"elapsed":4116,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}}},"source":["import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","\n","import MNISTtools"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P3pJZEgKgKQj"},"source":["Let us first load and normalize MNIST testing and training data. This dataset is made of 60000 grayscale images of size 28$\\times$28"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-4R4pD7gKQk","executionInfo":{"status":"ok","timestamp":1619273928312,"user_tz":-120,"elapsed":704,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}},"outputId":"4166b55a-da2a-48e1-dfbf-fe5591444eee"},"source":["x_train, y_train = MNISTtools.load(dataset='training')\n","x_test, y_test = MNISTtools.load(dataset='testing')\n","print(np.mean(x_train))\n","print(np.std(x_train))\n","#MNISTtools.show(x_train[:,0])\n","#print(y_train[0])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["33.318421449829934\n","78.56748998339798\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_r062EHfgKQl"},"source":["To reduce the computation cost, we only select a small part of $n$ images for further analysis. _You might modify this value later to visualise its influence on the results._"]},{"cell_type":"code","metadata":{"id":"5EEV3hIjgKQm","executionInfo":{"status":"ok","timestamp":1619273931383,"user_tz":-120,"elapsed":432,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}}},"source":["n = 5000\n","x_train = x_train[:,1:n+1]\n","y_train = y_train[1:n+1]"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RVAQGV29gKQm"},"source":["## Question 1\n","\n","How many training and testing images compose the dataset? What are the dimensions of the data samples?"]},{"cell_type":"markdown","metadata":{"id":"jekT8Xh3gKQs"},"source":["*COMPLETE*"]},{"cell_type":"markdown","metadata":{"id":"5q0Ez6SFgKQs"},"source":["To visualize the images, the vectors must be reshaped to a grayscale square image.  "]},{"cell_type":"code","metadata":{"id":"qkyzwHyjgKQt","colab":{"base_uri":"https://localhost:8080/","height":548},"executionInfo":{"status":"ok","timestamp":1619273934750,"user_tz":-120,"elapsed":1174,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}},"outputId":"aafa2eaf-28ef-4cc1-ee99-5eb6206a670d"},"source":["x_test_im = x_test.reshape((28, 28, 1, -1))\n","\n","x_train_im = x_train.reshape((28, 28, 1, -1))\n","\n","# let us show image number 42 and its label\n","MNISTtools.show(x_train_im[:, :, 0, 42])\n","print(\"the label of image 42 is\", y_train[42])\n","\n","# let us show image number 200 and its label\n","MNISTtools.show(x_train_im[:, :, 0, 200])\n","print(\"the label of image 200 is\", y_train[200])"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["the label of image 42 is 9\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["the label of image 200 is 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_WhVxBiGgKQt"},"source":["# PART 1 - K-Means\n","\n","In this first part you must group the testing set into 10 clusters using the K-Means algorithm. \n","You must observe the clustering result by visualizing as images the computed centroids and by checking if each found cluster corresponds to one label from the ground truth given by `y_test`. In other words, check to which cluster is assigned each data point, as in a classification problem. Analyse the results. \n","\n","Be careful: the index of the clusters you find (named `pred_labels` here) does not mandatorily correspond to the digit value (initial label `y_test`). One way to find which label corresponds to each cluster is to visualise the centroid image, or to compute the mode of each class in the ground truth, for example using the following code:\n","\n","```\n","from scipy.stats import mode\n","real_pred_labels = np.zeros_like(pred_labels)\n","real_pred_centers = np.zeros_like(pred_centers)\n","for i in range(10):\n","    indices = np.where(pred_labels == i)[0]\n","    real_value = mode(y_test[indices])[0]\n","    real_pred_labels[indices] = real_value\n","    real_pred_centers[real_value] = pred_centers[i]\n","```"]},{"cell_type":"code","metadata":{"id":"jOegnQAfgKQt","executionInfo":{"status":"ok","timestamp":1619273940055,"user_tz":-120,"elapsed":422,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}}},"source":["# Transpose data, necessary for K-Means, Naive Bayes and SVM with sklearn\n","x_test_t =  np.transpose(x_test)\n","x_train_t =  np.transpose(x_train)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"35VZ2RmIgKQt"},"source":["## Question 2\n","\n","Implement K-Means. Add comments to your code as necessary to make it more explicit"]},{"cell_type":"code","metadata":{"id":"X3gydo4agKQu"},"source":["# COMPLETE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LgDZhqvxgKQu"},"source":["## Question 3\n","\n","Analyse the results"]},{"cell_type":"code","metadata":{"id":"aofDIwURgKQv"},"source":["# COMPLETE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gmd8cz3bgKQv"},"source":["*COMPLETE*"]},{"cell_type":"markdown","metadata":{"id":"OYoXyKrJgKQ3"},"source":["# PART 2 - Naive Bayes\n","Apply Naive Bayes classification to the digits classification problem. Train the model on the training dataset and evaluate on the testing set. Analyse your results"]},{"cell_type":"markdown","metadata":{"id":"1JSxEiE0gKQ4"},"source":["## Question 4\n","\n","Implement Naive Bayes. Add comments to your code as necessary to make it more explicit"]},{"cell_type":"code","metadata":{"id":"-SAl-m_6gKQ4"},"source":["# COMPLETE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AXrTKTwngKQ5"},"source":["## Question 5\n","\n","Analyse the results obtained on the testing dataset "]},{"cell_type":"code","metadata":{"id":"Wg7ADXH3gKQ5"},"source":["# COMPLETE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ru0H4J3gKQ5"},"source":["*COMPLETE*"]},{"cell_type":"markdown","metadata":{"id":"y8r1K-jzgKQ5"},"source":["# PART 3 - SVM\n","Apply SVM classification with linear and polynomial/gaussian kernels to the digits classification problem. Train the models on the training dataset and evaluate on the testing set. Analyse your results"]},{"cell_type":"markdown","metadata":{"id":"sRzgtFdfgKQ6"},"source":["## Question 6\n","\n","Implement SVM classification with linear and polynomial/gaussian kernels. Add comments to your code as necessary to make it more explicit"]},{"cell_type":"code","metadata":{"id":"k5DFz4pugKQ6"},"source":["# COMPLETE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5tIogXPgKQ7"},"source":["## Question 7\n","\n","Analyse the results obtained on the testing dataset"]},{"cell_type":"code","metadata":{"id":"gH4KGvdTgKQ7"},"source":["# COMPLETE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"li6QJYMygKQ-"},"source":["*COMPLETE*"]},{"cell_type":"markdown","metadata":{"id":"ntPQCt3ggKQ-"},"source":["# PART 4 - CNNs\n","\n","CNNs are specific artificial neural networks composed of *convolutional* layers, *maxpooling* operations, and\n","*fully connected* layers.\n","- Convolutional layers are like typical layers where the weight matrix has a specific structure that is relevant for signals and images.  \n","They take as input $N$ images and produce as output $C$ images (called *feature maps* or *channels*).   \n","They are parameterized by a collection of coeficients that defines a filter bank. Each filter performs a weighted average of its inputs within local sliding windows of size $K \\times K$  (pixels) where $K$ is a hyperparameter (a small odd number: 3, 5, 7, 9).  \n","As for classical layers in neural networks, each feature map is next processed by an activation function such as  ReLU.  \n","    \n","- Maxpooling operations reduce the dimensions of the feature maps by picking the maximum value within local but non-overlapping sliding windows of size $L \\times L$ (pixels) where $L$ is another hyper-parameter (usually 2). Maxpooling does not introduce new parameters to be learned.  \n","  \n","- Fully connected layers are standard layers where the weight matrix does not have a specific structure: each of the $N$ output units is connected to each of the $M$ input units."]},{"cell_type":"markdown","metadata":{"id":"vox_yqZrgKQ_"},"source":["## Question 8\n","\n","PyTorch expects that the input of a convolutional layer is stored in the following format:\n","  $$\n","  \\texttt{Batch size} \\times \\texttt{Number of input channels} \\times \\texttt{Image height} \\times \\texttt{Image width}\n","  $$\n","  \n","The number of input channels in our case is 1 because MNIST is composed of grayscale images. It would have been 3 if the images were in RGB color.\n","In deeper layers, the number of input channels will be the number of feature maps coming from the previous layer.\n","\n","Reorganise the tensors `x_train_im` and `x_test_im` accordingly.\n","Hint:\n","  Reshape them first with shape $\\texttt{(28, 28, 1, 60000)}$ and $\\texttt{(28, 28, 1, 10000)}$\n","  respectively and then use `np.moveaxis`."]},{"cell_type":"code","metadata":{"id":"N6kJCKDfgKQ_","executionInfo":{"status":"ok","timestamp":1619273946297,"user_tz":-120,"elapsed":424,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}}},"source":["x_train_im = np.moveaxis(x_train_im, [0, 1, 2], [-1, -2, -3])\n","x_test_im = np.moveaxis(x_test_im, [0, 1, 2], [-1, -2, -3])"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N53lhKmJgKQ_"},"source":["Let us also normalize MNIST testing and training data."]},{"cell_type":"code","metadata":{"id":"-Ef3SYpfgKRA","executionInfo":{"status":"ok","timestamp":1619273947939,"user_tz":-120,"elapsed":404,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}}},"source":["def normalize_MNIST_images(x):\n","    return 2 * x.astype(np.float32) / 255. - 1\n","x_train_im = normalize_MNIST_images(x_train_im)\n","x_test_im = normalize_MNIST_images(x_test_im)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iScapAj6gKRA"},"source":["and finally wrap all the data into torch Tensors"]},{"cell_type":"code","metadata":{"id":"Bnw9XmTTgKRA","executionInfo":{"status":"ok","timestamp":1619273951881,"user_tz":-120,"elapsed":404,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}}},"source":["x_train_torch = torch.from_numpy(x_train_im)\n","y_train_torch = torch.from_numpy(y_train)\n","x_test_torch = torch.from_numpy(x_test_im)\n","y_test_torch = torch.from_numpy(y_test)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tc0O16lIgKRA"},"source":["## Question 9\n","\n","Neural networks can be constructed using the `torch.nn` package, which relies on `autograd` differentiation tools.\n","This package provides an implementation of CNNs as follows:\n","\n","* Convolutional layers can be created as `nn.Conv2d(N, C, K)`.\n","  For input images of size $W \\times H$, without padding the output feature maps have size $[W-K+1] \\times [H-K+1]$.\n","* In PyTorch, maxpooling is implemented like any other non-linear function (such as\n","  `ReLU` or `softmax`).\n","  For input images of size $W \\times H$, the output feature maps\n","  have size $\\lceil W/L \\rceil \\times \\lceil H/L \\rceil$.\n","* A fully connected layer can be created as `nn.Linear(M, N)`.\n","\n","Our LeNet network will be composed successively of\n","   1. a convolutional layer (i) connecting the input image to 6\n","    feature maps with $5 \\times 5$ convolutions ($K = 5$) and followed\n","    by ReLU and maxpooling (ii) ($L=2$),\n","   2. a convolutional layer (iii) connecting the 6 input channels to 16\n","    output channels with $5 \\times 5$ convolutions and followed\n","    by ReLU and maxpooling (iv) ($L=2$),\n","   3. a fully-connected layer connecting $16$ feature maps\n","    to $120$ output units and followed by ReLU,\n","   4. a fully-connected layer connecting $120$ inputs\n","    to $84$ output units and followed by ReLU,\n","   5. a final linear layer connecting $84$ inputs\n","    to $10$ linear outputs (one for each of our digits).\n","\n","Determine the size of the feature maps after each convolution and maxpooling operation i.e. at points (i)-(iv) processing steps. "]},{"cell_type":"markdown","metadata":{"id":"891rKqhKgKRB"},"source":["Lets consider one sample of size $28 \\times 28$.\n","\n","$(i)$ Applying $6$ filters of size $5 \\times 5$ with no padding and a stride of one would give us $6$ feature maps of size $24 \\times 24$ (taking into account the bias, this corresponds to $156$ parameters). \n","\n","$(ii)$ Following the maxpooling, each of the 6 feature maps will be reduced to $12 \\times 12$.\n","\n","$(iii)$ Considering a stride of one and no padding, we will obtain $16$ feature map each of size $8 \\times 8$ (this layer has $416$ parameters).\n","\n","$(iv)$ Following the maxpooling, each of the 16 feature maps will be reduced to $4 \\times 4$."]},{"cell_type":"markdown","metadata":{"id":"Ztn4dK-tgKRB"},"source":["## Question 10\n","\n","Analyse and complete the following code initializing our LeNet network. \n","\n","Note that you just have to define the forward function, and the backward function (where gradients are computed) will be automatically defined for you using `autograd`.\n","You can use any of the Torch tensor operations in the forward function.\n","For more details, please refer to https://pytorch.org/docs/stable/nn.html"]},{"cell_type":"code","metadata":{"id":"n4NF4TmmgKRC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619273956500,"user_tz":-120,"elapsed":723,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}},"outputId":"67076d31-0db7-4edb-d474-327319ff7cc3"},"source":["# The neural networks class\n","class LeNet(nn.Module):\n","\n","    # define our network structure\n","    def __init__(self):\n","        super(LeNet, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 6 , 5)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(256, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    # define one forward pass through the network\n","    def forward(self, x):\n","        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n","        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n","        x = x.view(-1, self.num_flat_features(x))\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","    \n","    # helper function to understand the dimensions\n","    def num_flat_features(self, x):\n","        size = x.size()[1:] # all dimensions except the batch dimension\n","        return np.prod(size)\n","\n","net = LeNet()\n","print(net)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["LeNet(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=256, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lg6WJmOwgKRC"},"source":["## Question 11\n","\n","Run the following and interpret the results. What are the learnable parameters?  Are gradients going to be tracked for all parameters?"]},{"cell_type":"code","metadata":{"id":"S74cB355gKRC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619273959188,"user_tz":-120,"elapsed":439,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}},"outputId":"53afe924-a5c2-4aa3-e7e4-2c1a61a5c7ac"},"source":["for name, param in net.named_parameters():\n","    print(name, param.size(), param.requires_grad)  "],"execution_count":12,"outputs":[{"output_type":"stream","text":["conv1.weight torch.Size([6, 1, 5, 5]) True\n","conv1.bias torch.Size([6]) True\n","conv2.weight torch.Size([16, 6, 5, 5]) True\n","conv2.bias torch.Size([16]) True\n","fc1.weight torch.Size([120, 256]) True\n","fc1.bias torch.Size([120]) True\n","fc2.weight torch.Size([84, 120]) True\n","fc2.bias torch.Size([84]) True\n","fc3.weight torch.Size([10, 84]) True\n","fc3.bias torch.Size([10]) True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yK6vl-2RgKRD"},"source":["This shows that the learnable parameters (weights and biases) of each layer.\n","\n","We can see that the gradients are going to be tracked for all parameters (**todo** elaborate here on why that is necessary for backprop).\n","\n","**Remark:** Although the size of the weight tensor in the second convolutional layer is $16\\times 6\\times 5\\times 5$ the real number of parameters is only $16\\times 5\\times 5$ because the $5\\times 5$ filters are broadcasted to $5\\times 5\\times 6$."]},{"cell_type":"markdown","metadata":{"id":"Kyg38PbZgKRD"},"source":["### Running a foward pass\n","\n","To run a forward pass of your initial network over your testing set, simply run the following code. \n","\n","Note that `with torch.no_grad()` is used to avoid tracking for\n","gradient during testing and then save some computation time\n","(refer to https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)."]},{"cell_type":"code","metadata":{"id":"UZi-PWXngKRE","executionInfo":{"status":"ok","timestamp":1619273965150,"user_tz":-120,"elapsed":1333,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}}},"source":["with torch.no_grad():\n","    pred = net(x_test_torch) # equivalent to pred = net.forward(x_test_torch)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4qCzvHtegKRE"},"source":["## Question 12\n","\n","Run the following and interpret the result"]},{"cell_type":"code","metadata":{"id":"H7br-w0wgKRF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619273967306,"user_tz":-120,"elapsed":386,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}},"outputId":"121e4c63-8a69-4bfc-b669-f79759d763c9"},"source":["_, pred_labels = pred.max(1)\n","print(100 * (y_test_torch == pred_labels).float().mean())"],"execution_count":14,"outputs":[{"output_type":"stream","text":["tensor(10.0500)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZXzde-BEgKRF"},"source":["`pred.max(1)` returns the index (between 0 to 9) having the maximum value for each image of the test set. This index can be interpreted as the most likely digit represented by the image.\n","\n","The initial values given to the parameters of the network allowed us to obtain an accuracy of $10.28\\%$. "]},{"cell_type":"markdown","metadata":{"id":"Dfamsa-HgKRF"},"source":["## Question 13\n","\n","We will use (Mini-Batch) Stochastic Gradient Descent (SGD) with momentum, and cross-entropy as the loss.\n","Complete the following function.\n","\n","For more details, refer to\n","https://pytorch.org/docs/stable/nn.html and\n","https://pytorch.org/docs/stable/optim.html.\n","\n","Note that PyTorch's `CrossEntropyLoss` is already the composition of a softmax activation with the standard cross-entropy loss."]},{"cell_type":"code","metadata":{"id":"Av-6gslogKRG","executionInfo":{"status":"ok","timestamp":1619273974085,"user_tz":-120,"elapsed":459,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}}},"source":["# T: number of epochs\n","# B: minibatch size, \n","# gamma: step size,\n","# rho: momentum.\n","def backprop_deep(x_train, y_train, net, T, B=100, gamma=.001, rho=.9):\n","    N = x_train.size()[0]        # Training set size\n","    NB = N // B   # Number of minibatches\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=rho)\n","    \n","    loss_values = []\n","    for epoch in range(T):\n","        running_loss = 0.0\n","        shuffled_indices = np.random.permutation(range(N))\n","        for k in range(NB):\n","            # Extract k-th minibatch from xtrain and ltrain\n","            minibatch_indices = shuffled_indices[B*k:min(B*(k+1), N)]\n","            inputs = x_train[minibatch_indices, ...]\n","            labels = y_train[minibatch_indices, ...]\n","\n","            # Initialize the gradients to zero\n","            optimizer.zero_grad()\n","\n","            # Forward propogation\n","            outputs = net(inputs) \n","\n","            # Error evaluation\n","            loss = criterion(outputs, labels)\n","\n","            # Back propogation\n","            loss.backward()\n","\n","            # Optimize step\n","            optimizer.step()\n","\n","            # Compute and print statistics\n","            with torch.no_grad():\n","                running_loss += loss.item()*inputs.size(0)\n","  \n","            \n","        loss_values.append(running_loss/NB)\n","  \n","    plt.plot(loss_values)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qT_MGIvwgKRG"},"source":["## Question 14\n","\n","Run the function for 50 epochs, it may take several minutes. The number of epochs has to be adapted to reach convergence as most as possible.\n","The loss per minibatch should decay (it may take some time). Explain the curve."]},{"cell_type":"code","metadata":{"id":"MPJ6trIOgKRG","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1619274031759,"user_tz":-120,"elapsed":54734,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}},"outputId":"751fe2fe-54dc-4f50-9885-36300649793f"},"source":["backprop_deep(x_train_torch, y_train_torch, net, T=50)"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"HH9qxzO2gKRG"},"source":["*COMPLETE*"]},{"cell_type":"markdown","metadata":{"id":"_Ctq0chogKRG"},"source":["## Question 15 - Optional\n","\n","If you have a GPU, experiment the following and analyze the results"]},{"cell_type":"code","metadata":{"id":"5toU_zUxgKRH"},"source":["import time\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","if device is not 'cpu':\n","    net = LeNet().to(device)\n","    xtrain = xtrain.to(device)\n","    ltrain = ltrain.to(device)\n","    t = time.time()\n","    backprop_deep(xtrain, ltrain, net, T=50)\n","    print(time.time() - t)\n","    net = net.to('cpu')\n","    xtrain_torch = xtrain_torch.to('cpu')\n","    ltrain_torch = ltrain_torch.to('cpu')\n","else:\n","    print('Sorry no GPU')  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YHvzT4-EgKRH"},"source":["## Question 16\n","\n","Analyse the results obtained by applying the network to the testing dataset. "]},{"cell_type":"code","metadata":{"id":"CZab9pbdgKRI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619274063849,"user_tz":-120,"elapsed":1315,"user":{"displayName":"sohaib errabii","photoUrl":"","userId":"02539498648735437705"}},"outputId":"dbd64265-a7cb-4a70-8434-6dceb317215f"},"source":["pred = net(x_test_torch)\n","_, pred_labels = pred.max(1)\n","print(\"Accuracy: \" + str(100 * (y_test_torch == pred_labels).float().mean()))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Accuracy: tensor(97.2400)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3Wo0nlz6gKRI"},"source":["*COMPLETE*"]},{"cell_type":"markdown","metadata":{"id":"-rleGvPRgKRI"},"source":["# PART 5 - Comparison and conclusion"]},{"cell_type":"markdown","metadata":{"id":"86BkRiAcgKRI"},"source":["## Question 9\n","\n","Compare all results and conclude\n"]},{"cell_type":"code","metadata":{"id":"Squ7PAbxgKRJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rUWKDi3ygKRK"},"source":["*COMPLETE*"]}]}